{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prime-pharmacology",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blank-beauty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.7.1\n",
      "spacy version: 3.0.1\n",
      "torchtext version: 0.8.0\n",
      "torch lightning version: 1.1.7\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# core torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# torch ecosystem\n",
    "import torchtext\n",
    "from torchtext.datasets import WMT14\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext import data\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "assert torch.__version__ == '1.7.1'\n",
    "assert spacy.__version__ == '3.0.1'\n",
    "assert torchtext.__version__ == '0.8.0'\n",
    "assert pl.__version__ == '1.1.7'\n",
    "\n",
    "print(f'torch version: {torch.__version__}')\n",
    "print(f'spacy version: {spacy.__version__}')\n",
    "print(f'torchtext version: {torchtext.__version__}')\n",
    "print(f'torch lightning version: {pl.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "compliant-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source code\n",
    "class TranslationDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir: Path, batch_size: int, reduced: bool, english_tokenizer, german_tokenizer):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.reduced = reduced\n",
    "\n",
    "        # tokenizers\n",
    "        self.english_tokenizer = english_tokenizer\n",
    "        self.german_tokenizer = german_tokenizer\n",
    "\n",
    "    def _reduce_data(self, \n",
    "                     source_suffix='train.tok.clean.bpe.32000',\n",
    "                     target_suffix='train_reduced',\n",
    "                     max_sent_len=30, \n",
    "                     max_data_size=None\n",
    "                     ):\n",
    "        '''\n",
    "        '''\n",
    "        en_source = str(self.data_dir / 'wmt14' / source_suffix) + '.en'\n",
    "        de_source = str(self.data_dir / 'wmt14' / source_suffix) + '.de'\n",
    "\n",
    "        english_df = pd.read_csv(en_source, sep='\\n', header=None)\n",
    "        german_df = pd.read_csv(de_source, sep='\\n', header=None)\n",
    "        \n",
    "        df = pd.concat([english_df, german_df], axis=1)\n",
    "        df.columns = ['english', 'german']\n",
    "\n",
    "        # restrict dataframe size\n",
    "        if max_data_size:\n",
    "            df = df.sample(n=max_data_size)\n",
    "\n",
    "        # preprocessing : move out of this func\n",
    "        df.english = df.english.str.lower()\n",
    "        df.german = df.german.str.lower()\n",
    "\n",
    "        # remove very long sentence\n",
    "        df['english_sent_len'] = df.english.apply(self.english_tokenizer).agg(len)\n",
    "        df['german_sent_len'] = df.german.apply(self.german_tokenizer).agg(len)\n",
    "        df = df.query(f'english_sent_len <= {max_sent_len} and german_sent_len <= {max_sent_len}')\n",
    "\n",
    "        en_target = str(self.data_dir / 'wmt14' / target_suffix) + '.en'\n",
    "        de_target = str(self.data_dir/ 'wmt14' /  target_suffix) + '.de'\n",
    "\n",
    "        df['english'].to_csv(en_target, sep='\\n', header=None, index=False)\n",
    "        df['german'].to_csv(de_target, sep='\\n', header=None, index=False)\n",
    "\n",
    "        print(f'number of sentence pairs in reduced dataset = {len(df)}')\n",
    "    \n",
    "    def prepare_data(self, max_sent_len=30, max_train_data_size=1_000_000):\n",
    "        \"\"\"\n",
    "        Use this method to do things that might write to disk or that need to be done only from a single GPU in distributed settings.\n",
    "        Example:\n",
    "            - Download dataset\n",
    "            - Tokenize\n",
    "        \"\"\"\n",
    "        # download\n",
    "        WMT14.download(self.data_dir)\n",
    "\n",
    "        # limit\n",
    "        if self.reduced:\n",
    "            # reduce training\n",
    "            self._reduce_data(\n",
    "                max_sent_len=max_sent_len,\n",
    "                max_data_size=max_train_data_size)\n",
    "\n",
    "            # reduce validation\n",
    "            self._reduce_data(\n",
    "                source_suffix='newstest2013.tok.bpe.32000', \n",
    "                target_suffix='valid_reduced',\n",
    "                max_sent_len=max_sent_len,\n",
    "                max_data_size=None\n",
    "                )\n",
    "\n",
    "\n",
    "    def setup(self, src_vocab_max_size=50_000, trgt_vocab_max_size=50_000, stage=None):\n",
    "        '''\n",
    "        There are also data operations you might want to perform on every GPU. Use setup to do things like:\n",
    "        Example:\n",
    "            - count number of classes\n",
    "            - build vocabulary\n",
    "            - perform train/val/test splits\n",
    "            - apply transforms (defined explicitly in your datamodule or assigned in init)\n",
    "        '''\n",
    "        eos_token = '<eos>'\n",
    "        self.src_field = torchtext.data.Field(\n",
    "            tokenize=self.english_tokenizer,\n",
    "            eos_token=eos_token, \n",
    "            batch_first=True,\n",
    "            lower=True,\n",
    "            )\n",
    "        self.trgt_field = torchtext.data.Field(\n",
    "            tokenize=self.german_tokenizer,\n",
    "            eos_token=eos_token, \n",
    "            batch_first=True,\n",
    "            lower=True\n",
    "            )\n",
    "        \n",
    "        root = str(self.data_dir)\n",
    "        train_data = 'train_reduced' if self.reduced else 'train.tok.clean.bpe.32000'\n",
    "        valid_data = 'valid_reduced' if self.reduced else 'newstest2013.tok.bpe.32000'\n",
    "\n",
    "        self.train, self.valid, self.test = WMT14.splits(\n",
    "            exts=('.en', '.de'), \n",
    "            fields=(self.src_field, self.trgt_field), \n",
    "            root=root,\n",
    "            train=train_data,\n",
    "            validation=valid_data\n",
    "            )\n",
    "        self.src_field.build_vocab(self.train, max_size=src_vocab_max_size)\n",
    "        self.trgt_field.build_vocab(self.train, max_size=trgt_vocab_max_size)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "#         return DataLoader(self.train, self.batch_size)\n",
    "        return torchtext.data.BucketIterator(\n",
    "            dataset=self.train, \n",
    "            batch_size=self.batch_size,\n",
    "            sort_key=lambda x: torchtext.data.interleave_keys(len(x.src), len(x.trgt))\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "#         return DataLoader(self.valid, self.batch_size)\n",
    "        return torchtext.data.BucketIterator(\n",
    "            dataset=self.valid, \n",
    "            batch_size=self.batch_size,\n",
    "            sort_key=lambda x: torchtext.data.interleave_keys(len(x.src), len(x.trgt))\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "#         return DataLoader(self.test, self.batch_size)\n",
    "        return torchtext.data.BucketIterator(\n",
    "            dataset=self.test, \n",
    "            batch_size=self.batch_size,\n",
    "            sort_key=lambda x: torchtext.data.interleave_keys(len(x.src), len(x.trgt))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "standard-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def generate_square_subsequent_mask(size: int):\n",
    "    \"\"\"Generate a triangular (size, size) mask. From PyTorch docs.\"\"\"\n",
    "    mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "# positionla encoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic Attention-is-all-you-need positional encoding.\n",
    "    From PyTorch docs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# transformer\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Classic Transformer that both encodes and decodes.\n",
    "    \n",
    "    Prediction-time inference is done greedily.\n",
    "\n",
    "    NOTE: start token is hard-coded to be 0, end token to be 1. If changing, update predict() accordingly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, max_output_length: int, dim: int = 128):\n",
    "        super().__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.dim = dim\n",
    "        self.max_output_length = max_output_length\n",
    "        nhead = 4\n",
    "        num_layers = 4\n",
    "        dim_feedforward = dim\n",
    "\n",
    "        # Encoder part\n",
    "        self.embedding = nn.Embedding(num_classes, dim)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model=self.dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(d_model=self.dim, nhead=nhead, dim_feedforward=dim_feedforward),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Decoder part\n",
    "        self.y_mask = generate_square_subsequent_mask(self.max_output_length)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=nn.TransformerDecoderLayer(d_model=self.dim, nhead=nhead, dim_feedforward=dim_feedforward),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(self.dim, num_classes)\n",
    "\n",
    "        # It is empirically important to initialize weights properly\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "      \n",
    "    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input\n",
    "            x: (B, Sx) with elements in (0, C) where C is num_classes\n",
    "            y: (B, Sy) with elements in (0, C) where C is num_classes\n",
    "        Output\n",
    "            (B, C, Sy) logits\n",
    "        \"\"\"\n",
    "        encoded_x = self.encode(x)  # (Sx, B, E)\n",
    "        output = self.decode(y, encoded_x)  # (Sy, B, C)\n",
    "        return output.permute(1, 2, 0)  # (B, C, Sy)\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input\n",
    "            x: (B, Sx) with elements in (0, C) where C is num_classes\n",
    "        Output\n",
    "            (Sx, B, E) embedding\n",
    "        \"\"\"\n",
    "        x = x.permute(1, 0)  # (Sx, B, E)\n",
    "        x = self.embedding(x) * math.sqrt(self.dim)  # (Sx, B, E)\n",
    "        x = self.pos_encoder(x)  # (Sx, B, E)\n",
    "        x = self.transformer_encoder(x)  # (Sx, B, E)\n",
    "        return x\n",
    "\n",
    "    def decode(self, y: torch.Tensor, encoded_x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Input\n",
    "            encoded_x: (Sx, B, E)\n",
    "            y: (B, Sy) with elements in (0, C) where C is num_classes\n",
    "        Output\n",
    "            (Sy, B, C) logits\n",
    "        \"\"\"\n",
    "        y = y.permute(1, 0)  # (Sy, B)\n",
    "        y = self.embedding(y) * math.sqrt(self.dim)  # (Sy, B, E)\n",
    "        y = self.pos_encoder(y)  # (Sy, B, E)\n",
    "        Sy = y.shape[0]\n",
    "        y_mask = self.y_mask[:Sy, :Sy].type_as(encoded_x)  # (Sy, Sy)\n",
    "        output = self.transformer_decoder(y, encoded_x, y_mask)  # (Sy, B, E)\n",
    "        output = self.fc(output)  # (Sy, B, C)\n",
    "        return output\n",
    "\n",
    "    def predict(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Method to use at inference time. Predict y from x one token at a time. This method is greedy\n",
    "        decoding. Beam search can be used instead for a potential accuracy boost.\n",
    "\n",
    "        Input\n",
    "            x: (B, Sx) with elements in (0, C) where C is num_classes\n",
    "        Output\n",
    "            (B, C, Sy) logits\n",
    "        \"\"\"\n",
    "        encoded_x = self.encode(x)\n",
    "        \n",
    "        output_tokens = (torch.ones((x.shape[0], self.max_output_length))).type_as(x).long() # (B, max_length)\n",
    "        output_tokens[:, 0] = 0  # Set start token\n",
    "        for Sy in range(1, self.max_output_length):\n",
    "            y = output_tokens[:, :Sy]  # (B, Sy)\n",
    "            output = self.decode(y, encoded_x)  # (Sy, B, C)\n",
    "            output = torch.argmax(output, dim=-1)  # (Sy, B)\n",
    "            output_tokens[:, Sy] = output[-1:]  # Set the last output token\n",
    "        return output_tokens\n",
    "\n",
    "# lightning Model\n",
    "class LitModel(pl.LightningModule):\n",
    "    \"\"\"Simple PyTorch-Lightning model to train our Transformer.\"\"\"\n",
    "\n",
    "    def __init__(self, model, padding_index):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.loss = nn.CrossEntropyLoss(ignore_index=padding_index)\n",
    "\n",
    "    def training_step(self, batch, batch_ind):\n",
    "        x, y = batch.src, batch.trg\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        logits = self.model(x, y)\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_ind):\n",
    "        x, y = batch.src, batch.trg\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        logits = self.model(x, y)\n",
    "        loss = self.loss(logits, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "divided-simulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of sentence pairs in reduced dataset = 54929\n",
      "number of sentence pairs in reduced dataset = 1941\n"
     ]
    }
   ],
   "source": [
    "# define where the data would downloaded and accessed on the local file system\n",
    "data_dir = Path('.')\n",
    "\n",
    "def english_tokenizer(text):\n",
    "    return text.split(' ')\n",
    "    en_model = spacy.load('en_core_web_sm')\n",
    "    return [token.text for token in en_model.tokenizer(text)]\n",
    "def german_tokenizer(text):\n",
    "    return text.split(' ')\n",
    "    de_model = spacy.load('de_core_news_sm')\n",
    "    return [token.text for token in de_model.tokenizer(text)]\n",
    "\n",
    "# create data module\n",
    "batch_size = 64\n",
    "dm = TranslationDataModule(data_dir, batch_size, True, english_tokenizer, german_tokenizer)\n",
    "dm.prepare_data(max_train_data_size=1_00_000)\n",
    "dm.setup()\n",
    "\n",
    "source_vocab = dm.src_field.vocab\n",
    "target_vocab = dm.trgt_field.vocab\n",
    "target_vocab_size = len(dm.trgt_field.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-works",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | Transformer      | 6.5 M \n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "6.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.5 M     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f884efe62e144ce6b3981e6d1c117eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e54873b8a94859959247a52a3a0d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_from_checkpoint = False\n",
    "\n",
    "# create transformer model\n",
    "model = Transformer(num_classes=target_vocab_size, max_output_length=32)\n",
    "\n",
    "# create lightning model\n",
    "if load_from_checkpoint:\n",
    "    checkpoint_path= checkpoint_callback.best_model_path\n",
    "    print(f'loading pretrained model from {checkpoint_path}')\n",
    "    LitModel.load_from_checkpoint(checkpoint_path)\n",
    "else:\n",
    "    pad_index = source_vocab.stoi[dm.src_field.pad_token]\n",
    "    lit_model = LitModel(model, padding_index=pad_index)\n",
    "\n",
    "# create trainer\n",
    "early_stop_callback = pl.callbacks.EarlyStopping(monitor='val_loss')\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor='val_loss')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5, \n",
    "    gpus=2,\n",
    "    num_nodes=1,\n",
    "    callbacks=[early_stop_callback, checkpoint_callback], \n",
    "    progress_bar_refresh_rate=79,\n",
    "    distributed_backend='dp'\n",
    "    )\n",
    "\n",
    "# train\n",
    "trainer.fit(lit_model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_tokenizer = dm.english_tokenizer\n",
    "german_tokenizer = dm.german_tokenizer\n",
    "\n",
    "source_vocab = dm.src_field.vocab\n",
    "target_vocab = dm.trgt_field.vocab\n",
    "\n",
    "sentence = 'a republican strategy to counter the re-election of obama.'\n",
    "sentence = 'the country agreed on a common strategy'\n",
    "tokens = english_tokenizer(sentence)\n",
    "indices =  [source_vocab.stoi[token] for token in tokens]  + [source_vocab.stoi['<eos>']] \n",
    "x = torch.tensor(indices).unsqueeze(0)\n",
    "\n",
    "# get the model\n",
    "checkpoint_path = checkpoint_callback.best_model_path\n",
    "lit_model = LitModel.load_from_checkpoint(checkpoint_path)\n",
    "\n",
    "prediction = lit_model.model.predict(x)\n",
    "prediction = prediction.squeeze()\n",
    "target_tokens = []\n",
    "for index in prediction.numpy():\n",
    "    token = target_vocab.itos[index]\n",
    "    if token == dm.trgt_field.eos_token:\n",
    "        print('breaking')\n",
    "        break\n",
    "    target_tokens.append(token)\n",
    "target_sentence = ' '.join(target_tokens)\n",
    "target_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-values",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
